#!/usr/bin/env python3
"""
vLLM MoEæ¨¡å‹å¯åŠ¨è„šæœ¬
æ”¯æŒå¤šç§MoEæ¨¡å‹çš„è‡ªåŠ¨é…ç½®å’Œå¯åŠ¨
é’ˆå¯¹L40S GPUä¼˜åŒ–
"""

import os
import sys
import yaml
import argparse
import subprocess
from pathlib import Path
from typing import Dict, Any, List

class VLLMMoELauncher:
    def __init__(self, config_path: str = "configs/moe_models.yaml"):
        self.config_path = config_path
        self.config = self.load_config()
        self.validate_environment()
    
    def load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        try:
            with open(self.config_path, 'r', encoding='utf-8') as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"âŒ é…ç½®æ–‡ä»¶æœªæ‰¾åˆ°: {self.config_path}")
            sys.exit(1)
        except yaml.YAMLError as e:
            print(f"âŒ é…ç½®æ–‡ä»¶æ ¼å¼é”™è¯¯: {e}")
            sys.exit(1)
    
    def validate_environment(self):
        """éªŒè¯ç¯å¢ƒ"""
        print("ğŸ” éªŒè¯ç¯å¢ƒ...")
        
        # æ£€æŸ¥vLLM
        try:
            import vllm
            print(f"âœ… vLLMç‰ˆæœ¬: {vllm.__version__}")
        except ImportError:
            print("âŒ vLLMæœªå®‰è£…")
            sys.exit(1)
        
        # æ£€æŸ¥GPU
        try:
            import torch
            if not torch.cuda.is_available():
                print("âŒ CUDAä¸å¯ç”¨")
                sys.exit(1)
            
            gpu_count = torch.cuda.device_count()
            print(f"âœ… æ£€æµ‹åˆ° {gpu_count} ä¸ªGPU")
            
            for i in range(gpu_count):
                gpu_name = torch.cuda.get_device_name(i)
                gpu_memory = torch.cuda.get_device_properties(i).total_memory / 1024**3
                print(f"  GPU {i}: {gpu_name} ({gpu_memory:.1f}GB)")
                
        except Exception as e:
            print(f"âŒ GPUæ£€æŸ¥å¤±è´¥: {e}")
            sys.exit(1)
    
    def list_models(self):
        """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
        print("\nğŸ“‹ å¯ç”¨çš„MoEæ¨¡å‹:")
        print("=" * 60)
        
        for model_key, model_config in self.config['models'].items():
            print(f"\nğŸ¤– {model_key}")
            print(f"  æ¨¡å‹: {model_config['model_name']}")
            print(f"  å¤§å°: {model_config['model_size']}")
            print(f"  æ¨èGPUæ•°: {model_config['recommended_gpus']}")
            print(f"  æè¿°: {model_config['description']}")
    
    def get_model_config(self, model_key: str) -> Dict[str, Any]:
        """è·å–æ¨¡å‹é…ç½®"""
        if model_key not in self.config['models']:
            print(f"âŒ æœªçŸ¥æ¨¡å‹: {model_key}")
            self.list_models()
            sys.exit(1)
        
        return self.config['models'][model_key]
    
    def build_vllm_command(self, model_key: str, scenario: str = "production", 
                          custom_args: Dict[str, Any] = None) -> List[str]:
        """æ„å»ºvLLMå¯åŠ¨å‘½ä»¤"""
        model_config = self.get_model_config(model_key)
        scenario_config = self.config['deployment_scenarios'].get(scenario, {})
        global_config = self.config['global_settings']
        
        # åŸºç¡€å‘½ä»¤
        cmd = [
            "python", "-m", "vllm.entrypoints.openai.api_server",
            "--model", model_config['model_name']
        ]
        
        # å¹¶è¡Œé…ç½®
        if model_config.get('tensor_parallel_size', 1) > 1:
            cmd.extend(["--tensor-parallel-size", str(model_config['tensor_parallel_size'])])
        
        if model_config.get('pipeline_parallel_size', 1) > 1:
            cmd.extend(["--pipeline-parallel-size", str(model_config['pipeline_parallel_size'])])
        
        # å†…å­˜å’Œæ€§èƒ½é…ç½®
        gpu_memory_util = scenario_config.get('gpu_memory_utilization', 
                                            model_config.get('gpu_memory_utilization', 0.85))
        cmd.extend(["--gpu-memory-utilization", str(gpu_memory_util)])
        
        max_model_len = scenario_config.get('max_model_len', 
                                          model_config.get('max_model_len', 4096))
        cmd.extend(["--max-model-len", str(max_model_len)])
        
        max_num_seqs = scenario_config.get('max_num_seqs', 
                                         global_config.get('max_num_seqs', 256))
        cmd.extend(["--max-num-seqs", str(max_num_seqs)])
        
        # æ•°æ®ç±»å‹
        if model_config.get('dtype'):
            cmd.extend(["--dtype", model_config['dtype']])
        
        # é‡åŒ–
        if model_config.get('quantization'):
            cmd.extend(["--quantization", model_config['quantization']])
        
        # æ€§èƒ½ä¼˜åŒ–
        if global_config.get('enable_chunked_prefill'):
            cmd.append("--enable-chunked-prefill")
        
        if global_config.get('max_num_batched_tokens'):
            cmd.extend(["--max-num-batched-tokens", str(global_config['max_num_batched_tokens'])])
        
        # APIé…ç½®
        api_config = self.config['api_settings']
        cmd.extend([
            "--host", api_config['host'],
            "--port", str(api_config['port'])
        ])
        
        # è‡ªå®šä¹‰å‚æ•°
        if custom_args:
            for key, value in custom_args.items():
                if value is not None:
                    cmd.extend([f"--{key.replace('_', '-')}", str(value)])
        
        return cmd
    
    def start_model(self, model_key: str, scenario: str = "production", 
                   custom_args: Dict[str, Any] = None, dry_run: bool = False):
        """å¯åŠ¨æ¨¡å‹"""
        print(f"\nğŸš€ å¯åŠ¨æ¨¡å‹: {model_key}")
        print(f"ğŸ“‹ éƒ¨ç½²åœºæ™¯: {scenario}")
        
        model_config = self.get_model_config(model_key)
        print(f"ğŸ¤– æ¨¡å‹åç§°: {model_config['model_name']}")
        print(f"ğŸ’¾ æ¨¡å‹å¤§å°: {model_config['model_size']}")
        print(f"ğŸ® æ¨èGPUæ•°: {model_config['recommended_gpus']}")
        
        # æ„å»ºå‘½ä»¤
        cmd = self.build_vllm_command(model_key, scenario, custom_args)
        
        print(f"\nğŸ“ å¯åŠ¨å‘½ä»¤:")
        print(" ".join(cmd))
        
        if dry_run:
            print("\nğŸ” å¹²è¿è¡Œæ¨¡å¼ï¼Œä¸å®é™…å¯åŠ¨æœåŠ¡")
            return
        
        # è®¾ç½®ç¯å¢ƒå˜é‡
        env = os.environ.copy()
        env['CUDA_VISIBLE_DEVICES'] = ','.join(map(str, range(model_config['recommended_gpus'])))
        
        print(f"\nğŸŒ ç¯å¢ƒå˜é‡:")
        print(f"CUDA_VISIBLE_DEVICES={env['CUDA_VISIBLE_DEVICES']}")
        
        try:
            print(f"\nğŸ¯ å¯åŠ¨vLLMæœåŠ¡...")
            print("=" * 60)
            subprocess.run(cmd, env=env, check=True)
        except KeyboardInterrupt:
            print("\n\nâš ï¸ æœåŠ¡è¢«ç”¨æˆ·ä¸­æ–­")
        except subprocess.CalledProcessError as e:
            print(f"\nâŒ å¯åŠ¨å¤±è´¥: {e}")
            sys.exit(1)


def main():
    parser = argparse.ArgumentParser(description="vLLM MoEæ¨¡å‹å¯åŠ¨å™¨")
    parser.add_argument("--model", "-m", type=str, help="æ¨¡å‹åç§°")
    parser.add_argument("--scenario", "-s", type=str, default="production",
                       choices=["development", "production", "high_throughput", "long_context"],
                       help="éƒ¨ç½²åœºæ™¯")
    parser.add_argument("--list", "-l", action="store_true", help="åˆ—å‡ºå¯ç”¨æ¨¡å‹")
    parser.add_argument("--dry-run", action="store_true", help="å¹²è¿è¡Œï¼Œåªæ˜¾ç¤ºå‘½ä»¤ä¸æ‰§è¡Œ")
    parser.add_argument("--config", "-c", type=str, default="configs/moe_models.yaml",
                       help="é…ç½®æ–‡ä»¶è·¯å¾„")
    
    # è‡ªå®šä¹‰å‚æ•°
    parser.add_argument("--max-model-len", type=int, help="æœ€å¤§æ¨¡å‹é•¿åº¦")
    parser.add_argument("--max-num-seqs", type=int, help="æœ€å¤§åºåˆ—æ•°")
    parser.add_argument("--gpu-memory-utilization", type=float, help="GPUå†…å­˜åˆ©ç”¨ç‡")
    parser.add_argument("--tensor-parallel-size", type=int, help="å¼ é‡å¹¶è¡Œå¤§å°")
    parser.add_argument("--pipeline-parallel-size", type=int, help="æµæ°´çº¿å¹¶è¡Œå¤§å°")
    
    args = parser.parse_args()
    
    # åˆ›å»ºå¯åŠ¨å™¨
    launcher = VLLMMoELauncher(args.config)
    
    if args.list:
        launcher.list_models()
        return
    
    if not args.model:
        print("âŒ è¯·æŒ‡å®šæ¨¡å‹åç§°")
        launcher.list_models()
        return
    
    # æ„å»ºè‡ªå®šä¹‰å‚æ•°
    custom_args = {}
    for arg_name in ['max_model_len', 'max_num_seqs', 'gpu_memory_utilization',
                     'tensor_parallel_size', 'pipeline_parallel_size']:
        value = getattr(args, arg_name.replace('-', '_'))
        if value is not None:
            custom_args[arg_name] = value
    
    # å¯åŠ¨æ¨¡å‹
    launcher.start_model(args.model, args.scenario, custom_args, args.dry_run)


if __name__ == "__main__":
    main()
