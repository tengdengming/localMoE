#!/bin/bash
# LocalMoE LinuxæœåŠ¡å™¨éƒ¨ç½²è„šæœ¬ (ä»…vLLMå¼•æ“Ž)
# é€‚ç”¨äºŽå†…ç½‘æœåŠ¡å™¨çŽ¯å¢ƒ (376GB RAM, 128 CPU, 4x L40S GPU)
# æ³¨æ„ï¼šDeepSpeedå·²è¢«æ³¨é‡ŠæŽ‰ï¼Œåªä½¿ç”¨vLLMå¼•æ“Ž
#
# ä½¿ç”¨æ–¹æ³•:
#   chmod +x scripts/deploy_vllm_only.sh
#   ./scripts/deploy_vllm_only.sh
#   ./scripts/deploy_vllm_only.sh start

set -e

# é¢œè‰²å®šä¹‰
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m'

log_info() { echo -e "${BLUE}[INFO]${NC} $1"; }
log_success() { echo -e "${GREEN}[SUCCESS]${NC} $1"; }
log_warning() { echo -e "${YELLOW}[WARNING]${NC} $1"; }
log_error() { echo -e "${RED}[ERROR]${NC} $1"; }

echo "ðŸš€ LocalMoE ç®€åŒ–éƒ¨ç½² (ä»…vLLMå¼•æ“Ž)"
echo "é€‚ç”¨äºŽ: 376GB RAM, 128 CPU, 4x L40S GPU"
echo "æ³¨æ„: DeepSpeedå·²è¢«æ³¨é‡ŠæŽ‰"
echo ""

# æ£€æŸ¥ç³»ç»Ÿè¦æ±‚
check_requirements() {
    log_info "æ£€æŸ¥ç³»ç»Ÿè¦æ±‚..."
    
    # æ£€æŸ¥GPU
    if ! command -v nvidia-smi &> /dev/null; then
        log_error "æœªæ£€æµ‹åˆ°NVIDIA GPUé©±åŠ¨"
        exit 1
    fi
    
    # æ£€æŸ¥GPUæ•°é‡å’Œåž‹å·
    GPU_COUNT=$(nvidia-smi --list-gpus | wc -l)
    log_info "æ£€æµ‹åˆ° $GPU_COUNT å¼ GPU"
    nvidia-smi --query-gpu=name --format=csv,noheader,nounits | head -4
    
    # æ£€æŸ¥å†…å­˜
    TOTAL_MEM=$(free -g | awk '/^Mem:/{print $2}')
    log_info "ç³»ç»Ÿå†…å­˜: ${TOTAL_MEM}GB"
    
    # æ£€æŸ¥Python
    if ! command -v python3 &> /dev/null; then
        log_error "æœªæ£€æµ‹åˆ°Python3"
        exit 1
    fi
    
    PYTHON_VERSION=$(python3 --version)
    log_info "Pythonç‰ˆæœ¬: $PYTHON_VERSION"
    
    log_success "ç³»ç»Ÿè¦æ±‚æ£€æŸ¥å®Œæˆ"
}

# åˆ›å»ºPythonè™šæ‹ŸçŽ¯å¢ƒ
setup_python_env() {
    log_info "è®¾ç½®PythonçŽ¯å¢ƒ..."
    
    # åˆ›å»ºè™šæ‹ŸçŽ¯å¢ƒ
    if [ ! -d "venv" ]; then
        python3 -m venv venv
        log_success "è™šæ‹ŸçŽ¯å¢ƒåˆ›å»ºå®Œæˆ"
    else
        log_info "è™šæ‹ŸçŽ¯å¢ƒå·²å­˜åœ¨"
    fi
    
    # æ¿€æ´»è™šæ‹ŸçŽ¯å¢ƒ
    source venv/bin/activate
    
    # å‡çº§pip
    pip install --upgrade pip
    log_success "pipå‡çº§å®Œæˆ"
}

# å®‰è£…é¡¹ç›®ä¾èµ–
install_dependencies() {
    log_info "å®‰è£…é¡¹ç›®ä¾èµ–..."
    
    source venv/bin/activate
    
    # å®‰è£…PyTorch (CUDA 12.1)
    log_info "å®‰è£…PyTorch..."
    pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
    
    # å®‰è£…åŸºç¡€ä¾èµ–
    log_info "å®‰è£…åŸºç¡€ä¾èµ–..."
    pip install -r requirements.txt
    
    # å®‰è£…vLLM (è·³è¿‡DeepSpeed)
    log_info "å®‰è£…vLLMå¼•æ“Ž..."
    pip install vllm>=0.2.0
    
    # å®‰è£…ä¼˜åŒ–åº“
    log_info "å®‰è£…ä¼˜åŒ–åº“..."
    pip install \
        flash-attn \
        xformers \
        triton \
        pynvml \
        zstandard \
        fastapi \
        uvicorn \
        pydantic
    
    log_success "ä¾èµ–å®‰è£…å®Œæˆ"
}

# é…ç½®çŽ¯å¢ƒ
setup_environment() {
    log_info "é…ç½®çŽ¯å¢ƒ..."
    
    # åˆ›å»ºå¿…è¦ç›®å½•
    mkdir -p logs models checkpoints data
    
    # åˆ›å»ºçŽ¯å¢ƒå˜é‡æ–‡ä»¶
    cat > .env << EOF
# LocalMoE çŽ¯å¢ƒå˜é‡ (ä»…vLLM)
LOCALMOE_ENVIRONMENT=production
LOCALMOE_HOST=0.0.0.0
LOCALMOE_PORT=8000
LOCALMOE_WORKERS=1
LOCALMOE_GPU_COUNT=4
LOCALMOE_PREFERRED_ENGINE=vllm
LOCALMOE_MAX_CONCURRENT=100
LOCALMOE_LOG_LEVEL=info

# CUDAè®¾ç½®
CUDA_VISIBLE_DEVICES=0,1,2,3
NVIDIA_VISIBLE_DEVICES=all

# Pythonè·¯å¾„
PYTHONPATH=$(pwd)
EOF
    
    log_success "çŽ¯å¢ƒé…ç½®å®Œæˆ"
}

# æµ‹è¯•å®‰è£…
test_installation() {
    log_info "æµ‹è¯•å®‰è£…..."
    
    source venv/bin/activate
    
    # æµ‹è¯•CUDA
    log_info "æµ‹è¯•CUDA..."
    python3 -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'GPU count: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    for i in range(torch.cuda.device_count()):
        print(f'GPU {i}: {torch.cuda.get_device_name(i)}')
"
    
    # æµ‹è¯•vLLM
    log_info "æµ‹è¯•vLLM..."
    python3 -c "
try:
    import vllm
    print('âœ… vLLMå¯¼å…¥æˆåŠŸ')
except ImportError as e:
    print(f'âŒ vLLMå¯¼å…¥å¤±è´¥: {e}')
"
    
    # æµ‹è¯•é¡¹ç›®æ¨¡å—
    log_info "æµ‹è¯•é¡¹ç›®æ¨¡å—..."
    export PYTHONPATH=$(pwd)
    python3 -c "
try:
    from src.config.settings import load_settings
    print('âœ… é…ç½®æ¨¡å—å¯¼å…¥æˆåŠŸ')
    from src.core.inference import VLLMInferenceEngine
    print('âœ… vLLMå¼•æ“Žæ¨¡å—å¯¼å…¥æˆåŠŸ')
except ImportError as e:
    print(f'âŒ æ¨¡å—å¯¼å…¥å¤±è´¥: {e}')
"
    
    log_success "å®‰è£…æµ‹è¯•å®Œæˆ"
}

# å¯åŠ¨æœåŠ¡
start_service() {
    log_info "å¯åŠ¨LocalMoEæœåŠ¡..."
    
    source venv/bin/activate
    export PYTHONPATH=$(pwd)
    
    # åŠ è½½çŽ¯å¢ƒå˜é‡
    if [ -f .env ]; then
        export $(cat .env | grep -v '^#' | xargs)
    fi
    
    log_info "æœåŠ¡å¯åŠ¨ä¸­..."
    log_info "è®¿é—®åœ°å€: http://localhost:8000"
    log_info "APIæ–‡æ¡£: http://localhost:8000/docs"
    
    # å¯åŠ¨æœåŠ¡
    python -m src.api.main
}

# æ˜¾ç¤ºä½¿ç”¨è¯´æ˜Ž
show_usage() {
    echo ""
    log_success "ðŸŽ‰ éƒ¨ç½²å®Œæˆï¼"
    echo ""
    echo "æ‰‹åŠ¨å¯åŠ¨æœåŠ¡:"
    echo "  source venv/bin/activate"
    echo "  export PYTHONPATH=\$(pwd)"
    echo "  python -m src.api.main"
    echo ""
    echo "æˆ–è€…è¿è¡Œ:"
    echo "  ./scripts/deploy_vllm_only.sh start"
    echo ""
    echo "æœåŠ¡åœ°å€:"
    echo "  - APIæœåŠ¡: http://localhost:8000"
    echo "  - APIæ–‡æ¡£: http://localhost:8000/docs"
    echo "  - å¥åº·æ£€æŸ¥: http://localhost:8000/health"
}

# ä¸»å‡½æ•°
main() {
    check_requirements
    setup_python_env
    install_dependencies
    setup_environment
    test_installation
    show_usage
}

# å¤„ç†å‘½ä»¤è¡Œå‚æ•°
case "${1:-}" in
    "start")
        start_service
        ;;
    "test")
        source venv/bin/activate
        export PYTHONPATH=$(pwd)
        test_installation
        ;;
    *)
        main
        ;;
esac
