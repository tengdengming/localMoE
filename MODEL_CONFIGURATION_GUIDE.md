# LocalMoE æ¨¡å‹é…ç½®æŒ‡å—

## ğŸ¯ æ¦‚è¿°

LocalMoEé¡¹ç›®ä¸­çš„"ä¸“å®¶æ¨¡å‹"é…ç½®åˆ†ä¸ºä¸¤ä¸ªå±‚é¢ï¼š

1. **MoEæ¶æ„å±‚é¢**: 8ä¸ªä¸“å®¶ç½‘ç»œçš„åˆ†é…å’Œè·¯ç”± (å·²æ³¨é‡Šæ‰DeepSpeedéƒ¨åˆ†)
2. **vLLMæ¨ç†å±‚é¢**: å®é™…çš„é¢„è®­ç»ƒæ¨¡å‹åŠ è½½å’Œæ¨ç†

## ğŸ—ï¸ å½“å‰æ¶æ„çŠ¶æ€

### DeepSpeed MoE (å·²æ³¨é‡Šæ‰)
- âŒ **ä¸“å®¶åˆ†ç‰‡**: 8ä¸ªä¸“å®¶åˆ†å¸ƒåˆ°4å¼ GPU
- âŒ **åŠ¨æ€è·¯ç”±**: Top-Kä¸“å®¶é€‰æ‹©
- âŒ **å†…å­˜ç®¡ç†**: LRUä¸“å®¶äº¤æ¢

### vLLMæ¨ç†å¼•æ“ (å½“å‰ä½¿ç”¨)
- âœ… **æ¨¡å‹å¹¶è¡Œ**: 4å¼ GPUå¼ é‡å¹¶è¡Œ
- âœ… **æ‰¹å¤„ç†**: åŠ¨æ€æ‰¹å¤„ç†ä¼˜åŒ–
- âœ… **å†…å­˜ä¼˜åŒ–**: PagedAttention

## ğŸ“ æ¨¡å‹é…ç½®ä½ç½®

### 1. ä¸»é…ç½®æ–‡ä»¶: `configs/config.yaml`

```yaml
# æ¨¡å‹é…ç½®
model:
  num_experts: 8              # MoEä¸“å®¶æ•°é‡ (å·²æ³¨é‡Šæ‰)
  top_k_experts: 2           # æ¿€æ´»ä¸“å®¶æ•°é‡ (å·²æ³¨é‡Šæ‰)
  hidden_size: 768           # éšè—å±‚å¤§å°
  intermediate_size: 3072    # ä¸­é—´å±‚å¤§å°
  max_sequence_length: 2048  # æœ€å¤§åºåˆ—é•¿åº¦
  quantization_type: "fp16"  # é‡åŒ–ç±»å‹
  enable_compilation: true   # å¯ç”¨ç¼–è¯‘ä¼˜åŒ–

# vLLMé…ç½® (å®é™…ä½¿ç”¨çš„æ¨¡å‹)
vllm:
  model_name: "microsoft/DialoGPT-medium"  # ğŸ”¥ å®é™…æ¨¡å‹è·¯å¾„
  tensor_parallel_size: 4                   # 4å¼ GPUå¹¶è¡Œ
  gpu_memory_utilization: 0.9              # GPUå†…å­˜ä½¿ç”¨ç‡
  max_model_len: 2048                      # æœ€å¤§æ¨¡å‹é•¿åº¦
  block_size: 16                           # å†…å­˜å—å¤§å°
  swap_space: 4                            # äº¤æ¢ç©ºé—´(GB)
  max_num_batched_tokens: 8192             # æœ€å¤§æ‰¹å¤„ç†tokenæ•°
  enable_prefix_caching: true              # å¯ç”¨å‰ç¼€ç¼“å­˜
  quantization: null                       # é‡åŒ–æ–¹å¼
```

### 2. ä»£ç ä¸­çš„é…ç½®: `src/core/inference/vllm_engine.py`

```python
@dataclass
class VLLMConfig:
    # ğŸ”¥ å…³é”®é…ç½®é¡¹
    model_name: str = "microsoft/DialoGPT-medium"  # æ¨¡å‹è·¯å¾„
    tensor_parallel_size: int = 4                   # GPUå¹¶è¡Œæ•°
    gpu_memory_utilization: float = 0.9            # å†…å­˜ä½¿ç”¨ç‡
    max_model_len: int = 2048                      # æœ€å¤§é•¿åº¦
```

## ğŸ”§ ç¡¬ä»¶åŒ¹é…åˆ†æ

### å½“å‰ç¡¬ä»¶é…ç½®
- **GPU**: 4x NVIDIA L40S (48GB VRAM each)
- **å†…å­˜**: 376GB RAM
- **CPU**: 128æ ¸å¿ƒ

### é…ç½®åŒ¹é…åº¦

#### âœ… åŒ¹é…è‰¯å¥½çš„é…ç½®
```yaml
vllm:
  tensor_parallel_size: 4        # å®Œç¾åŒ¹é…4å¼ GPU
  gpu_memory_utilization: 0.9    # åˆç†åˆ©ç”¨48GBæ˜¾å­˜
  max_num_batched_tokens: 8192   # é€‚åˆL40Sçš„è®¡ç®—èƒ½åŠ›
  swap_space: 4                  # åˆç†çš„äº¤æ¢ç©ºé—´
```

#### âš ï¸ éœ€è¦è°ƒæ•´çš„é…ç½®
```yaml
vllm:
  model_name: "microsoft/DialoGPT-medium"  # è¾ƒå°æ¨¡å‹ï¼Œæœªå……åˆ†åˆ©ç”¨ç¡¬ä»¶
  max_model_len: 2048                      # å¯ä»¥å¢åŠ åˆ°4096æˆ–8192
```

## ğŸš€ æ¨èçš„ç¡¬ä»¶ä¼˜åŒ–é…ç½®

### 1. å¤§æ¨¡å‹é…ç½® (å……åˆ†åˆ©ç”¨L40S)

```yaml
# configs/config.yaml
vllm:
  model_name: "meta-llama/Llama-2-70b-chat-hf"  # 70Bå¤§æ¨¡å‹
  tensor_parallel_size: 4                        # 4å¼ GPUå¹¶è¡Œ
  gpu_memory_utilization: 0.85                  # ç¨å¾®ä¿å®ˆçš„å†…å­˜ä½¿ç”¨
  max_model_len: 4096                           # å¢åŠ åºåˆ—é•¿åº¦
  max_num_batched_tokens: 16384                 # å¢åŠ æ‰¹å¤„ç†èƒ½åŠ›
  block_size: 32                                # å¢åŠ å—å¤§å°
  swap_space: 8                                 # å¢åŠ äº¤æ¢ç©ºé—´
```

### 2. ä¸­ç­‰æ¨¡å‹é…ç½® (å¹³è¡¡æ€§èƒ½å’Œèµ„æº)

```yaml
vllm:
  model_name: "mistralai/Mistral-7B-Instruct-v0.2"  # 7Bæ¨¡å‹
  tensor_parallel_size: 2                            # 2å¼ GPUå¹¶è¡Œ
  gpu_memory_utilization: 0.9                       # é«˜å†…å­˜ä½¿ç”¨ç‡
  max_model_len: 8192                               # é•¿åºåˆ—æ”¯æŒ
  max_num_batched_tokens: 32768                     # å¤§æ‰¹å¤„ç†
```

### 3. å¤šæ¨¡å‹å¹¶è¡Œé…ç½®

```yaml
# å¯ä»¥åŒæ—¶è¿è¡Œå¤šä¸ªæ¨¡å‹å®ä¾‹
vllm_instances:
  - model_name: "codellama/CodeLlama-13b-Instruct-hf"
    tensor_parallel_size: 2
    gpu_devices: [0, 1]
  - model_name: "mistralai/Mistral-7B-Instruct-v0.2"
    tensor_parallel_size: 2
    gpu_devices: [2, 3]
```

## ğŸ“ æ¨¡å‹æ–‡ä»¶å­˜å‚¨

### 1. æœ¬åœ°æ¨¡å‹è·¯å¾„

```bash
# æ¨¡å‹å­˜å‚¨ç›®å½•
./models/
â”œâ”€â”€ microsoft/
â”‚   â””â”€â”€ DialoGPT-medium/
â”œâ”€â”€ meta-llama/
â”‚   â””â”€â”€ Llama-2-70b-chat-hf/
â”œâ”€â”€ mistralai/
â”‚   â””â”€â”€ Mistral-7B-Instruct-v0.2/
â””â”€â”€ codellama/
    â””â”€â”€ CodeLlama-13b-Instruct-hf/
```

### 2. é…ç½®æœ¬åœ°æ¨¡å‹

```yaml
# ä½¿ç”¨æœ¬åœ°æ¨¡å‹è·¯å¾„
vllm:
  model_name: "./models/meta-llama/Llama-2-70b-chat-hf"
  trust_remote_code: false
  load_format: "auto"
```

## ğŸ”„ æ¨¡å‹åˆ‡æ¢å’Œç®¡ç†

### 1. è¿è¡Œæ—¶æ¨¡å‹åˆ‡æ¢

```python
# é€šè¿‡APIåˆ‡æ¢æ¨¡å‹
curl -X POST "http://localhost:8000/v1/models/switch" \
  -H "Content-Type: application/json" \
  -d '{
    "model_name": "meta-llama/Llama-2-70b-chat-hf",
    "tensor_parallel_size": 4
  }'
```

### 2. å¤šæ¨¡å‹æœåŠ¡

```python
# å¯åŠ¨å¤šä¸ªæ¨¡å‹å®ä¾‹
python -m src.api.main --config configs/multi_model_config.yaml
```

## ğŸ›ï¸ æ€§èƒ½è°ƒä¼˜å»ºè®®

### 1. GPUå†…å­˜ä¼˜åŒ–

```yaml
vllm:
  gpu_memory_utilization: 0.85  # L40Så»ºè®®å€¼
  enable_prefix_caching: true   # å¯ç”¨ç¼“å­˜
  use_v2_block_manager: true    # ä½¿ç”¨v2å†…å­˜ç®¡ç†å™¨
```

### 2. æ‰¹å¤„ç†ä¼˜åŒ–

```yaml
vllm:
  max_num_batched_tokens: 16384  # æ ¹æ®GPUæ•°é‡è°ƒæ•´
  max_num_seqs: 256             # æœ€å¤§åºåˆ—æ•°
  enable_chunked_prefill: true   # å¯ç”¨åˆ†å—é¢„å¡«å……
```

### 3. é‡åŒ–é…ç½®

```yaml
vllm:
  quantization: "awq"           # AWQé‡åŒ– (æ¨è)
  # quantization: "gptq"        # GPTQé‡åŒ–
  # quantization: "squeezellm"  # SqueezeLLMé‡åŒ–
```

## ğŸ” ç›‘æ§å’Œè¯Šæ–­

### 1. æ£€æŸ¥å½“å‰é…ç½®

```bash
# æŸ¥çœ‹å½“å‰æ¨¡å‹ä¿¡æ¯
curl http://localhost:8000/v1/models

# æŸ¥çœ‹GPUä½¿ç”¨æƒ…å†µ
nvidia-smi

# æŸ¥çœ‹å†…å­˜ç»Ÿè®¡
curl http://localhost:8000/v1/stats/memory
```

### 2. æ€§èƒ½ç›‘æ§

```bash
# æŸ¥çœ‹æ¨ç†æ€§èƒ½
curl http://localhost:8000/v1/stats/performance

# å®æ—¶GPUç›‘æ§
watch -n 1 nvidia-smi
```

## ğŸš¨ å¸¸è§é—®é¢˜å’Œè§£å†³æ–¹æ¡ˆ

### 1. æ¨¡å‹åŠ è½½å¤±è´¥

```bash
# æ£€æŸ¥æ¨¡å‹è·¯å¾„
ls -la ./models/microsoft/DialoGPT-medium/

# æ£€æŸ¥æƒé™
chmod -R 755 ./models/

# é‡æ–°ä¸‹è½½æ¨¡å‹
python -c "from transformers import AutoModel; AutoModel.from_pretrained('microsoft/DialoGPT-medium')"
```

### 2. GPUå†…å­˜ä¸è¶³

```yaml
# é™ä½å†…å­˜ä½¿ç”¨ç‡
vllm:
  gpu_memory_utilization: 0.7
  tensor_parallel_size: 4
  swap_space: 8
```

### 3. æ€§èƒ½ä¸ä½³

```yaml
# ä¼˜åŒ–é…ç½®
vllm:
  enable_prefix_caching: true
  use_v2_block_manager: true
  enable_chunked_prefill: true
  max_chunked_prefill_tokens: 1024
```

## ğŸ“ æ€»ç»“

å½“å‰é¡¹ç›®é…ç½®ï¼š
- âœ… **vLLMå¼•æ“**: ä½¿ç”¨`microsoft/DialoGPT-medium`æ¨¡å‹
- âŒ **MoEä¸“å®¶**: DeepSpeedéƒ¨åˆ†å·²æ³¨é‡Šæ‰
- ğŸ”§ **ç¡¬ä»¶åŒ¹é…**: 4å¼ L40S GPUï¼Œé…ç½®åˆç†ä½†å¯ä¼˜åŒ–
- ğŸ“ˆ **ä¼˜åŒ–ç©ºé—´**: å¯ä½¿ç”¨æ›´å¤§æ¨¡å‹å……åˆ†åˆ©ç”¨ç¡¬ä»¶èµ„æº
