"""
æ¨¡æ‹Ÿæ¨ç†å¼•æ“ - ç”¨äºå¿«é€ŸéªŒè¯åŠŸèƒ½
"""

import asyncio
import time
import random
import uuid
from typing import Dict, List, Optional, Any
from datetime import datetime

from ...api.models import (
    InferenceRequest, InferenceResponse, InferenceResult,
    ExpertInfo, InferenceMode, SystemMetrics, ExpertStatus
)


class MockInferenceEngine:
    """æ¨¡æ‹Ÿæ¨ç†å¼•æ“"""
    
    def __init__(self):
        self.model_name = "LocalMoE-Mock-Engine"
        self.version = "1.0.0-mock"
        self.start_time = time.time()
        self.request_count = 0
        self.total_tokens = 0
        
        # æ¨¡æ‹Ÿä¸“å®¶ä¿¡æ¯
        self.experts = [
            ExpertInfo(expert_id=i, device_id=i % 4, load=0.0, memory_usage=0.0, active=True)
            for i in range(8)
        ]
        
        # æ¨¡æ‹Ÿå“åº”æ¨¡æ¿
        self.response_templates = {
            InferenceMode.TEXT_ONLY: [
                "åŸºäºæ‚¨çš„æ–‡æœ¬è¾“å…¥ '{input}', æˆ‘ç†è§£æ‚¨æƒ³è¦äº†è§£ç›¸å…³ä¿¡æ¯ã€‚è¿™æ˜¯LocalMoEç³»ç»Ÿçš„æ–‡æœ¬å¤„ç†å“åº”ã€‚",
                "æ‚¨æåˆ°äº† '{input}', è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„è¯é¢˜ã€‚LocalMoEçš„æ–‡æœ¬ä¸“å®¶ä¸ºæ‚¨æä¾›ä»¥ä¸‹è§è§£...",
                "å…³äº '{input}' çš„é—®é¢˜ï¼Œæ–‡æœ¬å¤„ç†ä¸“å®¶è®¤ä¸ºè¿™æ¶‰åŠåˆ°å¤šä¸ªæ–¹é¢çš„è€ƒè™‘ã€‚",
            ],
            InferenceMode.CODE_ONLY: [
                "# åŸºäºæ‚¨çš„ä»£ç è¾“å…¥ï¼ŒLocalMoEä»£ç ä¸“å®¶ä¸ºæ‚¨æä¾›ä»¥ä¸‹åˆ†æï¼š\n# {input}\n# è¿™æ®µä»£ç çš„åŠŸèƒ½æ˜¯...",
                "```python\n# ä»£ç åˆ†æç»“æœ\n# è¾“å…¥: {input}\n# LocalMoEä»£ç ä¸“å®¶å»ºè®®...\n```",
                "// ä»£ç å®¡æŸ¥ç»“æœ\n// åŸå§‹ä»£ç : {input}\n// å»ºè®®ä¼˜åŒ–...",
            ],
            InferenceMode.MULTIMODAL: [
                "å¤šæ¨¡æ€åˆ†æç»“æœï¼š\næ–‡æœ¬éƒ¨åˆ†: {text}\nä»£ç éƒ¨åˆ†: {code}\nLocalMoEèåˆäº†æ–‡æœ¬å’Œä»£ç ä¸“å®¶çš„è§è§£...",
                "ç»¼åˆåˆ†æï¼š\n- æ–‡æœ¬ç†è§£: {text}\n- ä»£ç åˆ†æ: {code}\n- èåˆå»ºè®®: ...",
            ],
            InferenceMode.AUTO: [
                "è‡ªåŠ¨æ¨¡å¼åˆ†æï¼šLocalMoEæ™ºèƒ½è·¯ç”±é€‰æ‹©äº†æœ€é€‚åˆçš„ä¸“å®¶ç»„åˆæ¥å¤„ç†æ‚¨çš„è¯·æ±‚ '{input}'ã€‚",
                "æ™ºèƒ½è·¯ç”±ç»“æœï¼šæ ¹æ®è¾“å…¥å†…å®¹ç‰¹å¾ï¼Œç³»ç»Ÿé€‰æ‹©äº†åˆé€‚çš„å¤„ç†ç­–ç•¥...",
            ]
        }
    
    async def initialize(self) -> bool:
        """åˆå§‹åŒ–å¼•æ“"""
        print("ğŸš€ åˆå§‹åŒ–æ¨¡æ‹Ÿæ¨ç†å¼•æ“...")
        await asyncio.sleep(1)  # æ¨¡æ‹Ÿåˆå§‹åŒ–æ—¶é—´
        print("âœ… æ¨¡æ‹Ÿæ¨ç†å¼•æ“åˆå§‹åŒ–å®Œæˆ")
        return True
    
    async def inference(self, request: InferenceRequest) -> InferenceResponse:
        """æ‰§è¡Œæ¨ç†"""
        start_time = time.time()
        self.request_count += 1
        
        # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ
        await asyncio.sleep(random.uniform(0.5, 2.0))
        
        # ç”Ÿæˆå“åº”
        generated_text = self._generate_response(request)
        
        # æ¨¡æ‹Ÿtokenè®¡æ•°
        input_tokens = len((request.text or "") + (request.code or "")) // 4
        output_tokens = len(generated_text) // 4
        self.total_tokens += input_tokens + output_tokens
        
        # æ¨¡æ‹Ÿä¸“å®¶ä½¿ç”¨
        experts_used = self._select_experts(request.mode)
        
        # æ›´æ–°ä¸“å®¶è´Ÿè½½
        for expert_id in experts_used:
            if expert_id < len(self.experts):
                self.experts[expert_id].load = random.uniform(0.3, 0.8)
        
        inference_time = time.time() - start_time
        
        # åˆ›å»ºæ¨ç†ç»“æœ
        result = InferenceResult(
            generated_text=generated_text,
            input_tokens=input_tokens,
            output_tokens=output_tokens,
            inference_time_ms=inference_time * 1000,
            experts_used=experts_used,
            model_info={
                "model_name": self.model_name,
                "version": self.version,
                "mode": request.mode.value,
                "engine": "mock"
            }
        )
        
        return InferenceResponse(
            success=True,
            request_id=request.request_id or str(uuid.uuid4()),
            result=result,
            error=None,
            timestamp=time.time()
        )
    
    def _generate_response(self, request: InferenceRequest) -> str:
        """ç”Ÿæˆæ¨¡æ‹Ÿå“åº”"""
        mode = request.mode
        templates = self.response_templates.get(mode, self.response_templates[InferenceMode.AUTO])
        template = random.choice(templates)
        
        if mode == InferenceMode.MULTIMODAL:
            return template.format(
                text=request.text or "æ— æ–‡æœ¬è¾“å…¥",
                code=request.code or "æ— ä»£ç è¾“å…¥"
            )
        else:
            input_text = request.text or request.code or "æ— è¾“å…¥"
            return template.format(input=input_text[:100])
    
    def _select_experts(self, mode: InferenceMode) -> List[int]:
        """é€‰æ‹©ä¸“å®¶"""
        if mode == InferenceMode.TEXT_ONLY:
            return [0, 1]  # æ–‡æœ¬ä¸“å®¶
        elif mode == InferenceMode.CODE_ONLY:
            return [2, 3]  # ä»£ç ä¸“å®¶
        elif mode == InferenceMode.MULTIMODAL:
            return [0, 1, 2, 3]  # å¤šæ¨¡æ€ä¸“å®¶
        else:  # AUTO
            return random.sample(range(8), k=random.randint(2, 4))
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        uptime = time.time() - self.start_time
        
        return {
            "engine": {
                "name": self.model_name,
                "version": self.version,
                "type": "mock",
                "uptime_seconds": uptime,
                "status": "running"
            },
            "requests": {
                "total": self.request_count,
                "rate_per_minute": self.request_count / (uptime / 60) if uptime > 0 else 0
            },
            "tokens": {
                "total": self.total_tokens,
                "rate_per_second": self.total_tokens / uptime if uptime > 0 else 0
            },
            "experts": {
                "total": len(self.experts),
                "active": sum(1 for e in self.experts if e.active),
                "average_load": sum(e.load for e in self.experts) / len(self.experts)
            },
            "performance": {
                "average_latency_ms": random.uniform(800, 1500),
                "throughput_tokens_per_second": random.uniform(50, 150)
            }
        }
    
    def get_models(self) -> List[Dict[str, Any]]:
        """è·å–æ¨¡å‹åˆ—è¡¨"""
        return [
            {
                "name": self.model_name,
                "version": self.version,
                "type": "mock",
                "status": "loaded",
                "parameters": {
                    "experts": 8,
                    "max_tokens": 2048,
                    "context_length": 4096
                },
                "capabilities": ["text", "code", "multimodal"],
                "loaded_at": datetime.fromtimestamp(self.start_time).isoformat()
            }
        ]
    
    def get_expert_status(self) -> List[ExpertStatus]:
        """è·å–ä¸“å®¶çŠ¶æ€"""
        return [
            ExpertStatus(
                expert_id=expert.expert_id,
                device_id=expert.device_id,
                status="active" if expert.active else "inactive",
                load=expert.load,
                memory_usage_gb=random.uniform(2.0, 8.0),
                request_count=random.randint(10, 100),
                avg_latency_ms=random.uniform(500, 1200)
            )
            for expert in self.experts
        ]
    
    async def shutdown(self):
        """å…³é—­å¼•æ“"""
        print("ğŸ›‘ å…³é—­æ¨¡æ‹Ÿæ¨ç†å¼•æ“...")
        await asyncio.sleep(0.5)
        print("âœ… æ¨¡æ‹Ÿæ¨ç†å¼•æ“å·²å…³é—­")


# å…¨å±€å®ä¾‹
mock_engine = MockInferenceEngine()
