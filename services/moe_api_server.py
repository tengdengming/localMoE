#!/usr/bin/env python3
"""
vLLM MoEæ¨¡å‹ FastAPIæœåŠ¡åŒ…è£…å™¨
æä¾›ç»Ÿä¸€çš„REST APIæ¥å£ï¼Œæ”¯æŒå¤šç§MoEæ¨¡å‹
"""

import os
import sys
import time
import asyncio
import logging
from typing import Dict, List, Optional, Any, AsyncGenerator
from contextlib import asynccontextmanager

import yaml
import uvicorn
from fastapi import FastAPI, HTTPException, BackgroundTasks
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel, Field

# vLLM imports
from vllm import LLM, SamplingParams
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.utils import random_uuid

# é…ç½®æ—¥å¿—
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

# å…¨å±€å˜é‡
engine: Optional[AsyncLLMEngine] = None
model_config: Dict[str, Any] = {}

# Pydanticæ¨¡å‹
class ChatMessage(BaseModel):
    role: str = Field(..., description="æ¶ˆæ¯è§’è‰²: system, user, assistant")
    content: str = Field(..., description="æ¶ˆæ¯å†…å®¹")

class ChatCompletionRequest(BaseModel):
    model: str = Field(default="current", description="æ¨¡å‹åç§°")
    messages: List[ChatMessage] = Field(..., description="å¯¹è¯æ¶ˆæ¯åˆ—è¡¨")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="é‡‡æ ·æ¸©åº¦")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="æ ¸é‡‡æ ·æ¦‚ç‡")
    top_k: int = Field(default=50, ge=1, description="Top-Ké‡‡æ ·")
    max_tokens: int = Field(default=1024, ge=1, le=8192, description="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¾“å‡º")
    stop: Optional[List[str]] = Field(default=None, description="åœæ­¢è¯åˆ—è¡¨")

class CompletionRequest(BaseModel):
    model: str = Field(default="current", description="æ¨¡å‹åç§°")
    prompt: str = Field(..., description="è¾“å…¥æç¤º")
    temperature: float = Field(default=0.7, ge=0.0, le=2.0, description="é‡‡æ ·æ¸©åº¦")
    top_p: float = Field(default=0.9, ge=0.0, le=1.0, description="æ ¸é‡‡æ ·æ¦‚ç‡")
    top_k: int = Field(default=50, ge=1, description="Top-Ké‡‡æ ·")
    max_tokens: int = Field(default=1024, ge=1, le=8192, description="æœ€å¤§ç”Ÿæˆé•¿åº¦")
    stream: bool = Field(default=False, description="æ˜¯å¦æµå¼è¾“å‡º")
    stop: Optional[List[str]] = Field(default=None, description="åœæ­¢è¯åˆ—è¡¨")
    n: int = Field(default=1, ge=1, le=5, description="ç”Ÿæˆæ•°é‡")

class ModelInfo(BaseModel):
    id: str
    object: str = "model"
    created: int
    owned_by: str = "vllm-moe"

class Usage(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str
    object: str = "chat.completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Usage

class CompletionResponse(BaseModel):
    id: str
    object: str = "text_completion"
    created: int
    model: str
    choices: List[Dict[str, Any]]
    usage: Usage

# åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†
@asynccontextmanager
async def lifespan(app: FastAPI):
    """åº”ç”¨ç”Ÿå‘½å‘¨æœŸç®¡ç†"""
    # å¯åŠ¨æ—¶åˆå§‹åŒ–
    logger.info("ğŸš€ å¯åŠ¨vLLM MoE APIæœåŠ¡...")
    await initialize_engine()
    yield
    # å…³é—­æ—¶æ¸…ç†
    logger.info("ğŸ›‘ å…³é—­vLLM MoE APIæœåŠ¡...")
    if engine:
        # è¿™é‡Œå¯ä»¥æ·»åŠ æ¸…ç†é€»è¾‘
        pass

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="vLLM MoE API Server",
    description="åŸºäºvLLMçš„MoEæ¨¡å‹APIæœåŠ¡",
    version="1.0.0",
    lifespan=lifespan
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

async def initialize_engine():
    """åˆå§‹åŒ–vLLMå¼•æ“"""
    global engine, model_config
    
    try:
        # ä»ç¯å¢ƒå˜é‡è·å–é…ç½®
        model_name = os.getenv("VLLM_MODEL", "mistralai/Mixtral-8x7B-Instruct-v0.1")
        tensor_parallel_size = int(os.getenv("VLLM_TENSOR_PARALLEL_SIZE", "2"))
        gpu_memory_utilization = float(os.getenv("VLLM_GPU_MEMORY_UTILIZATION", "0.85"))
        max_model_len = int(os.getenv("VLLM_MAX_MODEL_LEN", "8192"))
        max_num_seqs = int(os.getenv("VLLM_MAX_NUM_SEQS", "256"))
        dtype = os.getenv("VLLM_DTYPE", "bfloat16")
        
        logger.info(f"åˆå§‹åŒ–æ¨¡å‹: {model_name}")
        logger.info(f"å¼ é‡å¹¶è¡Œ: {tensor_parallel_size}")
        logger.info(f"GPUå†…å­˜åˆ©ç”¨ç‡: {gpu_memory_utilization}")
        
        # åˆ›å»ºå¼•æ“å‚æ•°
        engine_args = AsyncEngineArgs(
            model=model_name,
            tensor_parallel_size=tensor_parallel_size,
            gpu_memory_utilization=gpu_memory_utilization,
            max_model_len=max_model_len,
            max_num_seqs=max_num_seqs,
            dtype=dtype,
            trust_remote_code=True,
            enforce_eager=False,
            worker_use_ray=False,
            engine_use_ray=False,
        )
        
        # åˆ›å»ºå¼‚æ­¥å¼•æ“
        engine = AsyncLLMEngine.from_engine_args(engine_args)
        
        # ä¿å­˜æ¨¡å‹é…ç½®
        model_config = {
            "model_name": model_name,
            "tensor_parallel_size": tensor_parallel_size,
            "max_model_len": max_model_len,
            "dtype": dtype
        }
        
        logger.info("âœ… vLLMå¼•æ“åˆå§‹åŒ–å®Œæˆ")
        
    except Exception as e:
        logger.error(f"âŒ å¼•æ“åˆå§‹åŒ–å¤±è´¥: {e}")
        sys.exit(1)

def format_chat_prompt(messages: List[ChatMessage]) -> str:
    """æ ¼å¼åŒ–èŠå¤©æç¤º"""
    # ç®€å•çš„èŠå¤©æ ¼å¼ï¼Œå¯ä»¥æ ¹æ®å…·ä½“æ¨¡å‹è°ƒæ•´
    formatted_messages = []
    for msg in messages:
        if msg.role == "system":
            formatted_messages.append(f"System: {msg.content}")
        elif msg.role == "user":
            formatted_messages.append(f"User: {msg.content}")
        elif msg.role == "assistant":
            formatted_messages.append(f"Assistant: {msg.content}")
    
    formatted_messages.append("Assistant:")
    return "\n".join(formatted_messages)

async def generate_stream(request_id: str, prompt: str, sampling_params: SamplingParams) -> AsyncGenerator[str, None]:
    """æµå¼ç”Ÿæˆ"""
    async for request_output in engine.generate(prompt, sampling_params, request_id):
        for output in request_output.outputs:
            yield output.text

@app.get("/")
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "vLLM MoE API Server",
        "model": model_config.get("model_name", "unknown"),
        "status": "running"
    }

@app.get("/v1/models")
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return {
        "object": "list",
        "data": [
            ModelInfo(
                id=model_config.get("model_name", "current"),
                created=int(time.time()),
                owned_by="vllm-moe"
            )
        ]
    }

@app.post("/v1/chat/completions")
async def chat_completions(request: ChatCompletionRequest):
    """èŠå¤©è¡¥å…¨æ¥å£"""
    if not engine:
        raise HTTPException(status_code=503, detail="å¼•æ“æœªåˆå§‹åŒ–")
    
    # æ ¼å¼åŒ–æç¤º
    prompt = format_chat_prompt(request.messages)
    
    # åˆ›å»ºé‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=request.temperature,
        top_p=request.top_p,
        top_k=request.top_k,
        max_tokens=request.max_tokens,
        stop=request.stop,
        use_beam_search=False,
    )
    
    request_id = random_uuid()
    
    if request.stream:
        # æµå¼å“åº”
        async def stream_generator():
            async for text in generate_stream(request_id, prompt, sampling_params):
                chunk = {
                    "id": request_id,
                    "object": "chat.completion.chunk",
                    "created": int(time.time()),
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "delta": {"content": text},
                        "finish_reason": None
                    }]
                }
                yield f"data: {chunk}\n\n"
            
            # ç»“æŸæ ‡è®°
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(stream_generator(), media_type="text/plain")
    
    else:
        # éæµå¼å“åº”
        results = []
        async for request_output in engine.generate(prompt, sampling_params, request_id):
            results.append(request_output)
        
        if not results:
            raise HTTPException(status_code=500, detail="ç”Ÿæˆå¤±è´¥")
        
        final_output = results[-1]
        generated_text = final_output.outputs[0].text
        
        return ChatCompletionResponse(
            id=request_id,
            created=int(time.time()),
            model=request.model,
            choices=[{
                "index": 0,
                "message": {
                    "role": "assistant",
                    "content": generated_text
                },
                "finish_reason": "stop"
            }],
            usage=Usage(
                prompt_tokens=len(final_output.prompt_token_ids),
                completion_tokens=len(final_output.outputs[0].token_ids),
                total_tokens=len(final_output.prompt_token_ids) + len(final_output.outputs[0].token_ids)
            )
        )

@app.post("/v1/completions")
async def completions(request: CompletionRequest):
    """æ–‡æœ¬è¡¥å…¨æ¥å£"""
    if not engine:
        raise HTTPException(status_code=503, detail="å¼•æ“æœªåˆå§‹åŒ–")

    # åˆ›å»ºé‡‡æ ·å‚æ•°
    sampling_params = SamplingParams(
        temperature=request.temperature,
        top_p=request.top_p,
        top_k=request.top_k,
        max_tokens=request.max_tokens,
        stop=request.stop,
        n=request.n,
        use_beam_search=False,
    )

    request_id = random_uuid()

    if request.stream:
        # æµå¼å“åº”
        async def stream_generator():
            async for text in generate_stream(request_id, request.prompt, sampling_params):
                chunk = {
                    "id": request_id,
                    "object": "text_completion",
                    "created": int(time.time()),
                    "model": request.model,
                    "choices": [{
                        "index": 0,
                        "text": text,
                        "finish_reason": None
                    }]
                }
                yield f"data: {chunk}\n\n"

            yield "data: [DONE]\n\n"

        return StreamingResponse(stream_generator(), media_type="text/plain")

    else:
        # éæµå¼å“åº”
        results = []
        async for request_output in engine.generate(request.prompt, sampling_params, request_id):
            results.append(request_output)

        if not results:
            raise HTTPException(status_code=500, detail="ç”Ÿæˆå¤±è´¥")

        final_output = results[-1]
        choices = []

        for i, output in enumerate(final_output.outputs):
            choices.append({
                "index": i,
                "text": output.text,
                "finish_reason": "stop"
            })

        return CompletionResponse(
            id=request_id,
            created=int(time.time()),
            model=request.model,
            choices=choices,
            usage=Usage(
                prompt_tokens=len(final_output.prompt_token_ids),
                completion_tokens=sum(len(output.token_ids) for output in final_output.outputs),
                total_tokens=len(final_output.prompt_token_ids) + sum(len(output.token_ids) for output in final_output.outputs)
            )
        )

@app.get("/health")
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    if not engine:
        raise HTTPException(status_code=503, detail="å¼•æ“æœªå°±ç»ª")

    return {
        "status": "healthy",
        "model": model_config.get("model_name", "unknown"),
        "timestamp": int(time.time())
    }

@app.get("/metrics")
async def get_metrics():
    """è·å–æœåŠ¡æŒ‡æ ‡"""
    if not engine:
        raise HTTPException(status_code=503, detail="å¼•æ“æœªå°±ç»ª")

    # è¿™é‡Œå¯ä»¥æ·»åŠ æ›´è¯¦ç»†çš„æŒ‡æ ‡æ”¶é›†
    return {
        "model": model_config.get("model_name", "unknown"),
        "tensor_parallel_size": model_config.get("tensor_parallel_size", 1),
        "max_model_len": model_config.get("max_model_len", 4096),
        "dtype": model_config.get("dtype", "float16"),
        "uptime": int(time.time()),
        "status": "running"
    }

def main():
    """ä¸»å‡½æ•°"""
    import argparse

    parser = argparse.ArgumentParser(description="vLLM MoE APIæœåŠ¡å™¨")
    parser.add_argument("--host", type=str, default="0.0.0.0", help="æœåŠ¡å™¨åœ°å€")
    parser.add_argument("--port", type=int, default=8000, help="æœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--workers", type=int, default=1, help="å·¥ä½œè¿›ç¨‹æ•°")
    parser.add_argument("--log-level", type=str, default="info", help="æ—¥å¿—çº§åˆ«")

    args = parser.parse_args()

    # å¯åŠ¨æœåŠ¡å™¨
    uvicorn.run(
        "moe_api_server:app",
        host=args.host,
        port=args.port,
        workers=args.workers,
        log_level=args.log_level,
        access_log=True,
        reload=False
    )

if __name__ == "__main__":
    main()
