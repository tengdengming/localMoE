# LocalMoE LinuxæœåŠ¡å™¨éƒ¨ç½²æ£€æŸ¥æ¸…å•

## ğŸ¯ éƒ¨ç½²å‰å‡†å¤‡

### ç¡¬ä»¶è¦æ±‚ç¡®è®¤
- [ ] **GPU**: 4x NVIDIA L40S (48GB VRAM each)
- [ ] **å†…å­˜**: 376GB RAM
- [ ] **CPU**: 128æ ¸å¿ƒ
- [ ] **å­˜å‚¨**: 2TB+ NVMe SSD
- [ ] **ç½‘ç»œ**: åƒå…†ä»¥ä¸Šå†…ç½‘è¿æ¥

### è½¯ä»¶ç¯å¢ƒç¡®è®¤
- [ ] **æ“ä½œç³»ç»Ÿ**: Ubuntu 20.04+ æˆ– CentOS 8+
- [ ] **NVIDIAé©±åŠ¨**: å·²å®‰è£…ä¸”ç‰ˆæœ¬ >= 525.xx
- [ ] **CUDA**: 12.1+ (å¯é€‰ï¼ŒPyTorchä¼šè‡ªå¸¦)
- [ ] **Python**: 3.8+ (å»ºè®®3.10)
- [ ] **Git**: å·²å®‰è£…
- [ ] **ç½‘ç»œ**: å¯è®¿é—®PyPIå’ŒHuggingFace

## ğŸš€ å¿«é€Ÿéƒ¨ç½²æ­¥éª¤

### 1. ä¸Šä¼ ä»£ç åˆ°æœåŠ¡å™¨

```bash
# æ–¹æ³•1: Gitå…‹éš† (æ¨è)
git clone <your-repo-url> LocalMoE
cd LocalMoE

# æ–¹æ³•2: scpä¸Šä¼ 
scp -r LocalMoE/ user@server-ip:~/

# æ–¹æ³•3: rsyncåŒæ­¥
rsync -avz --exclude='venv' --exclude='__pycache__' LocalMoE/ user@server-ip:~/LocalMoE/
```

### 2. æ‰§è¡Œä¸€é”®éƒ¨ç½²

```bash
cd LocalMoE
chmod +x deploy_linux_server.sh
./deploy_linux_server.sh
```

### 3. å¯åŠ¨æœåŠ¡

```bash
# æ–¹æ³•1: å‰å°è¿è¡Œ (æµ‹è¯•ç”¨)
./deploy_linux_server.sh start

# æ–¹æ³•2: åå°æœåŠ¡ (ç”Ÿäº§ç”¨)
sudo systemctl start localmoe
sudo systemctl status localmoe
```

## ğŸ“‹ è¯¦ç»†éƒ¨ç½²æ­¥éª¤

### æ­¥éª¤1: ç¯å¢ƒæ£€æŸ¥

```bash
# æ£€æŸ¥GPU
nvidia-smi

# æ£€æŸ¥å†…å­˜
free -h

# æ£€æŸ¥ç£ç›˜
df -h

# æ£€æŸ¥CPU
nproc
lscpu
```

### æ­¥éª¤2: ç³»ç»Ÿä¾èµ–å®‰è£…

```bash
# Ubuntu/Debian
sudo apt-get update
sudo apt-get install -y build-essential python3-dev python3-pip python3-venv git wget curl htop nvtop tmux vim

# CentOS/RHEL
sudo yum update -y
sudo yum groupinstall -y "Development Tools"
sudo yum install -y python3-devel python3-pip git wget curl htop tmux vim
```

### æ­¥éª¤3: Pythonç¯å¢ƒè®¾ç½®

```bash
# åˆ›å»ºè™šæ‹Ÿç¯å¢ƒ
python3 -m venv venv
source venv/bin/activate

# å‡çº§pip
pip install --upgrade pip

# å®‰è£…PyTorch (CUDA 12.1)
pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121
```

### æ­¥éª¤4: é¡¹ç›®ä¾èµ–å®‰è£…

```bash
# å®‰è£…åŸºç¡€ä¾èµ–
pip install -r requirements.txt

# å®‰è£…vLLM (è·³è¿‡DeepSpeed)
pip install vllm>=0.2.0

# å®‰è£…ä¼˜åŒ–åº“
pip install flash-attn xformers triton pynvml zstandard transformers accelerate
```

### æ­¥éª¤5: é…ç½®ç¯å¢ƒ

```bash
# åˆ›å»ºç›®å½•
mkdir -p logs models checkpoints data

# è®¾ç½®ç¯å¢ƒå˜é‡
export PYTHONPATH=$(pwd)
export CUDA_VISIBLE_DEVICES=0,1,2,3
```

### æ­¥éª¤6: æµ‹è¯•å®‰è£…

```bash
# æµ‹è¯•CUDA
python -c "import torch; print(f'CUDA: {torch.cuda.is_available()}, GPUs: {torch.cuda.device_count()}')"

# æµ‹è¯•vLLM
python -c "import vllm; print('vLLM OK')"

# æµ‹è¯•é¡¹ç›®æ¨¡å—
python -c "from src.config.settings import load_settings; print('Config OK')"
```

## ğŸ”§ é…ç½®ä¼˜åŒ–

### L40S GPUä¼˜åŒ–é…ç½®

ç¼–è¾‘ `configs/config.yaml`:

```yaml
# L40Sä¼˜åŒ–é…ç½®
vllm:
  model_name: "meta-llama/Llama-2-13b-chat-hf"  # æˆ–å…¶ä»–æ¨¡å‹
  quantization: "awq"                            # L40Sæ¨èé‡åŒ–
  tensor_parallel_size: 4                        # 4å¼ GPUå¹¶è¡Œ
  gpu_memory_utilization: 0.85                   # L40Sä¿å®ˆå†…å­˜ä½¿ç”¨
  max_model_len: 4096                           # é€‚ä¸­åºåˆ—é•¿åº¦
  max_num_batched_tokens: 16384                 # å¤§æ‰¹å¤„ç†
  enable_prefix_caching: true                   # åˆ©ç”¨å¤§æ˜¾å­˜
  use_v2_block_manager: true                    # v2å†…å­˜ç®¡ç†å™¨
```

### ç³»ç»Ÿä¼˜åŒ–

```bash
# GPUæ€§èƒ½æ¨¡å¼
sudo nvidia-smi -pm 1

# è®¾ç½®æœ€å¤§æ—¶é’Ÿé¢‘ç‡
sudo nvidia-smi -ac 6001,1410

# ä¼˜åŒ–ç³»ç»Ÿå‚æ•°
echo 'vm.swappiness=10' | sudo tee -a /etc/sysctl.conf
echo 'net.core.rmem_max=134217728' | sudo tee -a /etc/sysctl.conf
echo 'net.core.wmem_max=134217728' | sudo tee -a /etc/sysctl.conf
sudo sysctl -p
```

## ğŸŒ æœåŠ¡è®¿é—®

### APIç«¯ç‚¹

- **ä¸»é¡µ**: http://server-ip:8000/
- **APIæ–‡æ¡£**: http://server-ip:8000/docs
- **å¥åº·æ£€æŸ¥**: http://server-ip:8000/health
- **æ¨¡å‹åˆ—è¡¨**: http://server-ip:8000/models
- **æ¨ç†æ¥å£**: http://server-ip:8000/v1/inference

### æµ‹è¯•API

```bash
# å¥åº·æ£€æŸ¥
curl http://localhost:8000/health

# æ¨ç†æµ‹è¯•
curl -X POST "http://localhost:8000/v1/inference" \
  -H "Content-Type: application/json" \
  -d '{
    "text": "Hello, how are you?",
    "model_config": {
      "max_tokens": 100,
      "temperature": 0.7
    }
  }'
```

## ğŸ“Š ç›‘æ§å’Œç®¡ç†

### æœåŠ¡ç®¡ç†

```bash
# systemdæœåŠ¡ç®¡ç†
sudo systemctl start localmoe      # å¯åŠ¨
sudo systemctl stop localmoe       # åœæ­¢
sudo systemctl restart localmoe    # é‡å¯
sudo systemctl status localmoe     # çŠ¶æ€
sudo systemctl enable localmoe     # å¼€æœºè‡ªå¯

# æŸ¥çœ‹æ—¥å¿—
sudo journalctl -u localmoe -f     # å®æ—¶æ—¥å¿—
sudo journalctl -u localmoe -n 100 # æœ€è¿‘100è¡Œ
tail -f logs/localmoe.log          # åº”ç”¨æ—¥å¿—
```

### æ€§èƒ½ç›‘æ§

```bash
# GPUç›‘æ§
watch -n 1 nvidia-smi
nvtop

# ç³»ç»Ÿç›‘æ§
htop
iotop

# ç½‘ç»œç›‘æ§
netstat -tlnp | grep 8000
ss -tlnp | grep 8000
```

### æ—¥å¿—ç®¡ç†

```bash
# æ—¥å¿—è½®è½¬é…ç½®
sudo tee /etc/logrotate.d/localmoe << EOF
/path/to/LocalMoE/logs/*.log {
    daily
    missingok
    rotate 30
    compress
    delaycompress
    notifempty
    copytruncate
}
EOF
```

## ğŸš¨ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜

1. **GPUå†…å­˜ä¸è¶³**
   ```bash
   # æ£€æŸ¥GPUä½¿ç”¨æƒ…å†µ
   nvidia-smi
   
   # è°ƒæ•´é…ç½®
   # configs/config.yaml -> vllm.gpu_memory_utilization: 0.8
   ```

2. **ç«¯å£è¢«å ç”¨**
   ```bash
   # æ£€æŸ¥ç«¯å£
   netstat -tlnp | grep 8000
   
   # ä¿®æ”¹ç«¯å£
   # .env -> LOCALMOE_PORT=8001
   ```

3. **ä¾èµ–å†²çª**
   ```bash
   # é‡æ–°åˆ›å»ºç¯å¢ƒ
   rm -rf venv
   python3 -m venv venv
   source venv/bin/activate
   pip install -r requirements.txt
   ```

4. **æ¨¡å‹ä¸‹è½½å¤±è´¥**
   ```bash
   # æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹
   huggingface-cli download microsoft/DialoGPT-medium --local-dir ./models/DialoGPT-medium
   
   # é…ç½®æœ¬åœ°è·¯å¾„
   # configs/config.yaml -> vllm.model_name: "./models/DialoGPT-medium"
   ```

### æ€§èƒ½è°ƒä¼˜

1. **å†…å­˜ä¼˜åŒ–**
   - è°ƒæ•´ `gpu_memory_utilization`
   - å¯ç”¨ `enable_prefix_caching`
   - ä½¿ç”¨ `use_v2_block_manager`

2. **å¹¶å‘ä¼˜åŒ–**
   - è°ƒæ•´ `max_num_batched_tokens`
   - è®¾ç½®åˆé€‚çš„ `max_num_seqs`
   - ä¼˜åŒ– `tensor_parallel_size`

3. **é‡åŒ–ä¼˜åŒ–**
   - L40Sæ¨èä½¿ç”¨ `awq` æˆ– `fp8`
   - å¤§æ¨¡å‹å¿…é¡»ä½¿ç”¨é‡åŒ–
   - å°æ¨¡å‹å¯ä»¥ä¸é‡åŒ–

## âœ… éƒ¨ç½²å®Œæˆæ£€æŸ¥

- [ ] æœåŠ¡æ­£å¸¸å¯åŠ¨
- [ ] APIæ¥å£å“åº”æ­£å¸¸
- [ ] GPUæ­£å¸¸è¯†åˆ«å’Œä½¿ç”¨
- [ ] å†…å­˜ä½¿ç”¨åˆç†
- [ ] æ—¥å¿—æ­£å¸¸è¾“å‡º
- [ ] systemdæœåŠ¡é…ç½®æ­£ç¡®
- [ ] é˜²ç«å¢™ç«¯å£å¼€æ”¾
- [ ] æ€§èƒ½ç›‘æ§æ­£å¸¸

## ğŸ“ æŠ€æœ¯æ”¯æŒ

å¦‚æœé‡åˆ°é—®é¢˜ï¼Œè¯·æ£€æŸ¥ï¼š

1. **ç³»ç»Ÿæ—¥å¿—**: `sudo journalctl -u localmoe -f`
2. **åº”ç”¨æ—¥å¿—**: `tail -f logs/localmoe.log`
3. **GPUçŠ¶æ€**: `nvidia-smi`
4. **ç³»ç»Ÿèµ„æº**: `htop`
5. **ç½‘ç»œè¿æ¥**: `netstat -tlnp | grep 8000`

éƒ¨ç½²æˆåŠŸåï¼Œæ‚¨å°†æ‹¥æœ‰ä¸€ä¸ªé«˜æ€§èƒ½çš„LocalMoEæ¨ç†æœåŠ¡ï¼Œå……åˆ†åˆ©ç”¨L40S GPUçš„ç¡¬ä»¶ä¼˜åŠ¿ï¼
