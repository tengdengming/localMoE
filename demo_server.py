#!/usr/bin/env python3
"""
LocalMoE æ¼”ç¤ºæœåŠ¡å™¨ (CPUç‰ˆæœ¬)
é€‚ç”¨äºWindowsç¯å¢ƒä¸‹çš„åŠŸèƒ½æ¼”ç¤º
"""

import asyncio
import logging
import time
from typing import Dict, Any, List, Optional
from datetime import datetime

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from pydantic import BaseModel
import uvicorn

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('logs/demo_server.log'),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

# è¯·æ±‚å’Œå“åº”æ¨¡å‹
class InferenceRequest(BaseModel):
    text: str
    model_config: Optional[Dict[str, Any]] = {}
    max_tokens: Optional[int] = 100
    temperature: Optional[float] = 0.7

class InferenceResponse(BaseModel):
    generated_text: str
    model_used: str
    inference_time: float
    timestamp: str
    metadata: Dict[str, Any]

class HealthResponse(BaseModel):
    status: str
    version: str
    uptime: float
    system_info: Dict[str, Any]

class ModelInfo(BaseModel):
    name: str
    type: str
    status: str
    parameters: Dict[str, Any]

# æ¨¡æ‹Ÿçš„æ¨ç†å¼•æ“
class MockInferenceEngine:
    """æ¨¡æ‹Ÿæ¨ç†å¼•æ“ï¼Œç”¨äºæ¼”ç¤º"""
    
    def __init__(self):
        self.model_name = "LocalMoE-Demo-CPU"
        self.start_time = time.time()
        self.request_count = 0
        
        # æ¨¡æ‹Ÿçš„å“åº”æ¨¡æ¿
        self.response_templates = [
            "åŸºäºæ‚¨çš„è¾“å…¥ '{input}', æˆ‘ç†è§£æ‚¨æƒ³è¦äº†è§£ç›¸å…³ä¿¡æ¯ã€‚è¿™æ˜¯ä¸€ä¸ªæ¼”ç¤ºå“åº”ã€‚",
            "æ‚¨æåˆ°äº† '{input}', è¿™æ˜¯ä¸€ä¸ªæœ‰è¶£çš„è¯é¢˜ã€‚åœ¨LocalMoEç³»ç»Ÿä¸­ï¼Œæˆ‘ä»¬å¯ä»¥å¤„ç†å„ç§ç±»å‹çš„æŸ¥è¯¢ã€‚",
            "å…³äº '{input}' çš„é—®é¢˜ï¼Œæˆ‘å¯ä»¥ä¸ºæ‚¨æä¾›ä»¥ä¸‹è§è§£ï¼šè¿™æ˜¯ä¸€ä¸ªCPUç‰ˆæœ¬çš„æ¼”ç¤ºæœåŠ¡ã€‚",
            "æ„Ÿè°¢æ‚¨çš„è¾“å…¥ '{input}'ã€‚LocalMoEæ¼”ç¤ºç³»ç»Ÿæ­£åœ¨ä¸ºæ‚¨ç”Ÿæˆå“åº”...",
        ]
    
    async def generate(self, text: str, **kwargs) -> str:
        """æ¨¡æ‹Ÿæ–‡æœ¬ç”Ÿæˆ"""
        self.request_count += 1
        
        # æ¨¡æ‹Ÿæ¨ç†å»¶è¿Ÿ
        await asyncio.sleep(0.5)
        
        # é€‰æ‹©å“åº”æ¨¡æ¿
        template_idx = self.request_count % len(self.response_templates)
        template = self.response_templates[template_idx]
        
        # ç”Ÿæˆå“åº”
        response = template.format(input=text[:50])
        
        # æ·»åŠ ä¸€äº›éšæœºå†…å®¹
        if kwargs.get('max_tokens', 100) > 50:
            response += f"\n\nè¿™æ˜¯ç¬¬ {self.request_count} æ¬¡æ¨ç†è¯·æ±‚ã€‚"
            response += f"å½“å‰æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}"
        
        return response
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–å¼•æ“ç»Ÿè®¡ä¿¡æ¯"""
        return {
            "model_name": self.model_name,
            "uptime": time.time() - self.start_time,
            "total_requests": self.request_count,
            "status": "running",
            "engine_type": "mock_cpu"
        }

# åˆ›å»ºFastAPIåº”ç”¨
app = FastAPI(
    title="LocalMoE Demo Server",
    description="LocalMoE æ¼”ç¤ºæœåŠ¡å™¨ (CPUç‰ˆæœ¬)",
    version="1.0.0-demo"
)

# æ·»åŠ CORSä¸­é—´ä»¶
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# å…¨å±€å˜é‡
inference_engine = MockInferenceEngine()
server_start_time = time.time()

@app.on_event("startup")
async def startup_event():
    """æœåŠ¡å¯åŠ¨äº‹ä»¶"""
    logger.info("ğŸš€ LocalMoE Demo Server å¯åŠ¨ä¸­...")
    logger.info("ğŸ“ è¿è¡Œç¯å¢ƒ: Windows CPU")
    logger.info("âš ï¸  æ³¨æ„: è¿™æ˜¯æ¼”ç¤ºç‰ˆæœ¬ï¼ŒDeepSpeedå·²è¢«æ³¨é‡Šæ‰")
    logger.info("âœ… æœåŠ¡å¯åŠ¨å®Œæˆ")

@app.on_event("shutdown")
async def shutdown_event():
    """æœåŠ¡å…³é—­äº‹ä»¶"""
    logger.info("ğŸ›‘ LocalMoE Demo Server æ­£åœ¨å…³é—­...")

@app.get("/", response_model=Dict[str, str])
async def root():
    """æ ¹è·¯å¾„"""
    return {
        "message": "LocalMoE Demo Server",
        "status": "running",
        "version": "1.0.0-demo",
        "docs": "/docs"
    }

@app.get("/health", response_model=HealthResponse)
async def health_check():
    """å¥åº·æ£€æŸ¥"""
    uptime = time.time() - server_start_time
    
    return HealthResponse(
        status="healthy",
        version="1.0.0-demo",
        uptime=uptime,
        system_info={
            "platform": "windows",
            "engine": "mock_cpu",
            "deepspeed_enabled": False,
            "vllm_enabled": False,
            "demo_mode": True
        }
    )

@app.get("/models", response_model=List[ModelInfo])
async def list_models():
    """åˆ—å‡ºå¯ç”¨æ¨¡å‹"""
    return [
        ModelInfo(
            name="LocalMoE-Demo-CPU",
            type="mock",
            status="ready",
            parameters={
                "max_tokens": 500,
                "temperature_range": [0.1, 2.0],
                "supports_streaming": False,
                "demo_mode": True
            }
        )
    ]

@app.post("/v1/inference", response_model=InferenceResponse)
async def inference(request: InferenceRequest):
    """æ¨ç†æ¥å£"""
    start_time = time.time()
    
    try:
        logger.info(f"æ”¶åˆ°æ¨ç†è¯·æ±‚: {request.text[:100]}...")
        
        # éªŒè¯è¾“å…¥
        if not request.text.strip():
            raise HTTPException(status_code=400, detail="è¾“å…¥æ–‡æœ¬ä¸èƒ½ä¸ºç©º")
        
        if len(request.text) > 1000:
            raise HTTPException(status_code=400, detail="è¾“å…¥æ–‡æœ¬è¿‡é•¿ (æœ€å¤§1000å­—ç¬¦)")
        
        # æ‰§è¡Œæ¨ç†
        generated_text = await inference_engine.generate(
            request.text,
            max_tokens=request.max_tokens,
            temperature=request.temperature
        )
        
        inference_time = time.time() - start_time
        
        logger.info(f"æ¨ç†å®Œæˆï¼Œè€—æ—¶: {inference_time:.2f}ç§’")
        
        return InferenceResponse(
            generated_text=generated_text,
            model_used="LocalMoE-Demo-CPU",
            inference_time=inference_time,
            timestamp=datetime.now().isoformat(),
            metadata={
                "request_length": len(request.text),
                "response_length": len(generated_text),
                "max_tokens": request.max_tokens,
                "temperature": request.temperature,
                "demo_mode": True
            }
        )
        
    except Exception as e:
        logger.error(f"æ¨ç†å¤±è´¥: {str(e)}")
        raise HTTPException(status_code=500, detail=f"æ¨ç†å¤±è´¥: {str(e)}")

@app.get("/v1/stats")
async def get_stats():
    """è·å–ç»Ÿè®¡ä¿¡æ¯"""
    engine_stats = inference_engine.get_stats()
    
    return {
        "server": {
            "uptime": time.time() - server_start_time,
            "version": "1.0.0-demo",
            "platform": "windows"
        },
        "engine": engine_stats,
        "system": {
            "demo_mode": True,
            "deepspeed_enabled": False,
            "vllm_enabled": False,
            "gpu_available": False
        }
    }

@app.get("/v1/config")
async def get_config():
    """è·å–é…ç½®ä¿¡æ¯"""
    return {
        "model": {
            "name": "LocalMoE-Demo-CPU",
            "type": "mock",
            "quantization": "none",
            "max_sequence_length": 1000
        },
        "inference": {
            "engine": "mock",
            "max_concurrent_requests": 10,
            "timeout": 30
        },
        "deployment": {
            "environment": "demo",
            "platform": "windows",
            "gpu_count": 0,
            "deepspeed_commented": True
        }
    }

if __name__ == "__main__":
    print("ğŸš€ å¯åŠ¨ LocalMoE Demo Server...")
    print("ğŸ“ è¿è¡Œç¯å¢ƒ: Windows CPU")
    print("âš ï¸  æ³¨æ„: DeepSpeedå·²è¢«æ³¨é‡Šæ‰ï¼Œè¿™æ˜¯æ¼”ç¤ºç‰ˆæœ¬")
    print("ğŸŒ æœåŠ¡åœ°å€: http://localhost:8000")
    print("ğŸ“š APIæ–‡æ¡£: http://localhost:8000/docs")
    
    # å¯åŠ¨æœåŠ¡
    uvicorn.run(
        "demo_server:app",
        host="0.0.0.0",
        port=8000,
        reload=False,
        log_level="info"
    )
